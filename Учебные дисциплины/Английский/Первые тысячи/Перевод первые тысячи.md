Большие языковые модели (Large Language Models, LLM) сегодня находятся _на пике популярности_, и почти невозможно не сталкиваться с новостной _истерией_ вокруг ChatGPT, BingGPT и Bard. Много говорится о достижении искусственного общего интеллекта (AGI), но стоит ли нам беспокоиться, что машины вскоре вытеснят людей из всех видов интеллектуального труда, включая перевод? Может ли машина действительно овладеть языком?

Применения машинного обучения к данным естественного языка находятся _в полном разгаре_ уже более пяти лет. В 2022 году исследования в области обработки естественного языка (NLP) сообщили о прорывах во множестве направлений, особенно в совершенствовании систем нейронного машинного перевода (NMT) и нейронных генераторов текста (NLG), таких как Generative _Pre-trained_ Transformer 3 (GPT-3) и ChatGPT — чат-ориентированная вариация GPT-3, способная создавать человекоподобный, хотя и _не всегда последовательный_, цифровой текст. Она предсказывает следующее слово по истории текста, и часто результат оказывается релевантным и полезным. Это происходит потому, что модель обучалась на миллиардах предложений и способна извлекать наиболее подходящий материал из увиденных данных, исходя из контекста запроса.

GPT-3 и другие LLM могут генерировать тексты, написанные алгоритмами, которые зачастую практически неотличимы от текстов, созданных человеком — предложений, абзацев, статей, рассказов и так далее. Они даже способны генерировать программный код, используя огромные массивы ранее виденных примеров. Это показывает, что такие системы могут быть полезны во многих бизнес-приложениях, связанных с текстом, и улучшать взаимодействие между предприятиями и клиентами, когда дело касается текстовой информации в различных формах.

Первоначальный ажиотаж и восторг вокруг GPT-3 вызвали множество аналогичных инициатив по всему миру. Сегодня видно, что огромный корпус из 175 миллиардов параметров, использованный при создании GPT-3, уже превзойдён несколькими ещё более крупными моделями — например, Gopher от DeepMind, имеющей 280 миллиардов параметров и демонстрирующей лучшие результаты по большинству тестов, используемых для оценки возможностей подобных систем.

**Совсем недавно** [**ChatGPT произвёл фурор во всём мире**](https://multilingual.com/tag/chatgpt), **и многие представители интеллектуальных профессий опасаются вытеснения на фоне этой волны ажиотажа, хотя мы видим те же проблемы у всех LLM: отсутствие здравого смысла, понимания, а также постоянную угрозу дезинформации и “галлюцинаций”. Не говоря уже о полном игнорировании вопросов конфиденциальности данных и авторского права при их создании.**

**Сведения о параметрах и данных обучения GPT-4 остаются засекреченными компанией OpenAI, которая теперь решила извлечь прибыль из “золотой лихорадки” вокруг LLM, но многие оценивают объём параметров в триллионы и более.**

**Стоит отметить, что первоначальная этическая позиция OpenAI теперь фактически утрачена, и возникает вопрос, воспринималась ли она вообще всерьёз. Их изначальная миссия гласила:**

> **_«Наша цель — развивать цифровой интеллект таким образом, который, с наибольшей вероятностью, принесёт пользу всему человечеству, не ограничиваясь необходимостью получать финансовую прибыль. Поскольку наши исследования не обременены финансовыми обязательствами, мы можем лучше сосредоточиться на положительном человеческом воздействии.»_**

**В эпоху, когда “чушь” окружает нас повсюду, почему это должно быть исключением?**

Ажиотаж вокруг так называемых «прорывных» возможностей закономерно вызывает вопросы о возрастающей роли языковых ИИ в всё более широком спектре интеллектуального труда. Увидим ли мы рост присутствия машин в областях, связанных с человеческим языком? Возможна ли замена человека машиной в растущем числе языковых профессий?

Текущая тенденция в развитии LLM — создание всё более крупных моделей в стремлении достичь новых высот, однако ни одна компания пока не провела строгий анализ, какие именно переменные влияют на мощность этих моделей. Они способны выдавать впечатляющие результаты, но при этом часто генерируют полностью неверные факты — так называемые “галлюцинации”, когда машина уверенно соединяет случайные текстовые элементы, создавая правдоподобную, но ложную информацию. Многие критики утверждают, что увеличение размеров моделей не решает выявленных проблем — фабрикации ложных фактов, отсутствия понимания и здравого смысла.

> **_«ChatGPT “написал” грамматически безупречный, но безжизненный текст. Он выдал столько ложных результатов поиска, что подорвал мою веру даже в те, что казались убедительными с первого взгляда. Он повторил шаблонные рассуждения о будущем искусственного интеллекта.»_**  
> — **_Трей Попп,_** [_Alien Minds, Immaculate Bullshit, Outstanding Questions_](https://thepenngazette.com/alien-minds-immaculate-bullshit-outstanding-questions/)

Первоначальная эйфория уступает место осознанию проблем, присущих LLM, и пониманию, что увеличение объёма данных и вычислительных мощностей не устранит токсичность и предвзятость. Критики утверждают, что масштабирование мало помогает в достижении “понимания”, и создание GPT-4 со 100 триллионами параметров, даже при колоссальных затратах, может не дать эффекта. Токсичность и предвзятость, присущие таким системам, невозможно преодолеть просто добавлением данных и вычислений — необходимы иные подходы, пока неясно какие, но многие считают, что это потребует выхода за рамки машинного обучения.

GPT-3 и другие LLM можно заставить генерировать ложный, расистский, сексистский и искажённый контент, лишённый здравого смысла. Результат модели зависит от входных данных: “мусор на входе — мусор на выходе”.

> **_«Просто успокойтесь насчёт GPT-4 и перестаньте путать производительность с компетентностью. То, в чём LLM действительно хороши, — это в том, чтобы говорить так, как будто ответ _звучит правильно_, что отличается от того, чтобы ответ _был правильным_.»_**  
> — [**Родни Брукс**](https://spectrum-ieee-org.cdn.ampproject.org/c/s/spectrum.ieee.org/amp/gpt-4-calm-down-2660261157)

Техники вроде обучения с подкреплением на основе человеческой обратной связи (RLHF) могут ограничить грубые ошибки, но сужают диапазон возможных правильных ответов. Многие отмечают, что этот метод не способен устранить все проблемы алгоритмически создаваемого текста из-за огромного числа непредсказуемых ситуаций.

Если вникнуть глубже, становится очевидно, что, хотя текст модели грамматически корректен и часто идиоматичен, её понимание мира серьёзно искажено. Ей нельзя полностью доверять. Повсеместное распространение ненадёжного ИИ способно привести к масштабным социальным проблемам.

Несмотря на способность LLM иногда создавать тексты, схожие с человеческими, алгоритмы машинного обучения по сути остаются сложными математическими функциями, сопоставляющими наблюдения с результатами. Они могут предсказывать закономерности, ранее встречавшиеся в данных, на которых обучались. Следовательно, они эффективны лишь в пределах этих данных и начинают давать сбои, когда реальные данные отклоняются от обучающих примеров.

В декабре 2021 года произошёл инцидент с Amazon Alexa: она посоветовала ребёнку фактически ударить себя током (дотронуться до розетки монетой) в рамках “игры с вызовом”. Этот случай — как и многие другие — демонстрирует, что языковые ИИ не обладают пониманием и здравым смыслом, а их рекомендации могут быть абсурдны и даже опасны.

Гэри Маркус прокомментировал это так:

> **_«Ни один современный ИИ даже отдалённо не понимает повседневный физический или психологический мир. То, что у нас есть, — лишь приближение к интеллекту, а не сам интеллект, и потому ему нельзя доверять.»_**

Крупные статистические модели, обученные на огромных массивах данных, могут выполнять почти любые задачи на уровне демонстрации концепции, но надёжно — очень немногие, поскольку у них отсутствуют необходимые фундаментальные основы.

Таким образом, в сообществе ИИ растёт признание того, что язык действительно является сложной проблемой — и её нельзя решить только увеличением данных и усложнением алгоритмов. Это не значит, что такие системы бесполезны: наоборот, их польза очевидна, но использовать их следует осторожно, с человеческим контролем — по крайней мере до тех пор, пока машины не приобретут более надёжное понимание и здравый смысл.

Мы уже видим, что машинный перевод (MT) стал повсеместным и, по оценкам, обеспечивает 99,5% всего объёма переводов на планете ежедневно. Однако используется он главным образом для перевода огромных объёмов кратковременных или второстепенных текстов, которые никогда бы не переводились вручную. Ежедневно переводятся триллионы слов, но там, где перевод действительно важен, всегда присутствует человеческий контроль — особенно если ошибка может повлечь серьёзные последствия.

Хотя сферы применения машинного обучения стремительно расширяются, растёт и осознание того, что участие человека (“human-in-the-loop”) зачастую необходимо, так как машина лишена понимания, мышления и здравого смысла.

> **_Как сказал Родни Брукс, сооснователь iRobot, в статье «Неудобная правда об ИИ»:_**
> 
> **_«Практически каждое успешное внедрение ИИ включает одно из двух: либо где-то в системе есть человек, либо цена ошибки, если система ошибётся, крайне мала.»_**

---

**Что делает человеческий язык столь сложным для машинного обучения?**

Участники сообщества сингулярности сформулировали проблему достаточно точно. Они признают, что “язык — это трудно”, объясняя, почему [ИИ всё ещё не овладел переводом](https://multilingual.com/issues/february-2023/the-great-gap-will-mt-ever-be-on-par-with-human-translators/). Машины лучше всего справляются с задачами, имеющими бинарные (чёткие) результаты.

[Майкл Хаусман, преподаватель Университета Сингулярности,](https://singularityhub.com/2018/03/04/why-hasnt-ai-mastered-language-translation/) пояснил, что идеальный сценарий для машинного обучения — это ситуация с фиксированными правилами и ясными критериями успеха или неудачи. В качестве примера он назвал шахматы и отметил, что машины сумели обыграть лучших игроков в Го быстрее, чем ожидалось, именно из-за чётких правил и ограниченного набора возможных ходов.